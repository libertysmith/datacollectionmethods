[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CV/Resume",
    "section": "",
    "text": "Liberty “Libby” Smith\n“Knowledge will give you power, but character respect.” - Bruce Lee\n\nEDUCATION\n\nThe University of Texas at Dallas, (M.S., Social Data Analytics and Research) December 2024\n\nHypothesis testing, Predictive modeling, clustering, R & Python\n\nThe University of Texas at Dallas, (Bachelor of Arts in Political Science) May 2017\n\n\n\nRELEVANT EXPERIENCE\nSuitsupply, Dallas, Texas, USA ( Web Sales and Customer Service Analyst) Dec 2021 - Present\n\nProvide growth-oriented b2c reporting focusing on their e-commerce KPIs, growth campaigns, and brick&mortar customer journeys.\nReport quantitative business intelligence and visualizations via R and Python, discovered insights using store reviews, salesforce, and product data. IE: text/sentiment analysis of store reviews, client segmentation, and customer churn forecasting.\nFacilitated training on product knowledge and store services. Monitored store performance, execution, and conflict resolution.\nSalesForce developer functions including data integrity of b2c customer engagement, customization, customer service flow, and automation of repetitive tasks.\nProject management, task triaging, and delegation.\nCollaboration across mulitple departments to address complaint management and client relations.\nEarned the rating of ‘Exceptional’ for 2 consecutive years and recognized among the top performers in the branch office.\n\nKorea Economic Institute of America, Washington, D.C., USA (Economic Intern) Sept 2018 - Jan 2019\n\nLed and assisted in foreign policy pieces published on “The Peninsula” – our think-tank-sponsored blog – alongside legacy media journalists and diplomats. Read more here\nCollaborate with stakeholders gauged the value of Korea-US foreign trade and social policy recommendations. Advocated policy directions for equitable outcomes in economic and social matters.\nSocial media management and organizational reporting of engagement metrics on Twitter, Facebook, SproutSocial, and SurveyMonkey.\n\n\n\nEXTRA-CURRICULAR ACTIVITIES\n\nActs of Kindness at UTD – Founder and Treasurer Fall 2014 - Spring 2016\n\nWith a team of 12 members, proposed budgets, organized the Council events, led presentations to advocate awareness on campus, coordinated Acts of Kindness drives supporting students struggling financially during exam weeks.\nPromoted inter and intra-college participation by strategizing existing modes of communication, created online presence, and multiplied reach utilizing social media.\n\n\n\n\nADDITIONAL INFORMATION\nTechnical Skills: MySQL Certificate, R, HTML, CSS, Google Analytics, Adobe Analytics, SalesForce Developer rights, Python, MS Project Professional, MS Office, Agile Methodology, Project Management, Predictive and Statistical modeling, probability modeling, research and brief writing.\nLanguages: Advanced English and Conversational Korean\nEligibility: U.S. Citizen Authorized to work in the United States"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Analyze a Qualtrics Survey Template and Improve it",
    "section": "",
    "text": "Analyze a Qualtrics Survey Template and Improve it\nThis Movie Rental Behaviors Survey Template is analyzed for the purposes of assessing its internal and external validity. You can take or review the unedited survey here.\n\nHow is the survey structured?\n\n\nWhat is it composed of?\n\n\nHow are they ordered?\nIn this comprehensive survey, we delve into the nuances of movie renting preferences and habits, leveraging the power of data analytics to extract meaningful insights. Our survey comprises a total of 18 crafted Qualtrics questions, each strategically designed to uncover the fascinating layers of consumer behavior.\nThe journey begins with the first question, which unfolds into a captivating narrative with 12 sub-questions, presented with the Likert scale responses. These sub-questions serve as our compass, guiding us through the myriad of choices and preferences that shape the modern movie-watching experience. From preferred movie ratings to rental frequency, we explore it all, giving each response a quantitative weight that illuminates patterns and trends.\nDiving deeper, we encounter binary questions, offering respondents a simple yet impactful choice between “yes” and “no.” These questions serve as data points that punctuate our survey, allowing us to draw clear distinctions and make informed decisions based on dichotomous preferences.\nWe venture into the realm of ordinal and nominal multiple-choice responses, steering away from traditional Likert scales to gather unique insights that go beyond the conventional. These responses are the paintbrush strokes on our canvas, adding color and texture to the rich tapestry of movie-watching habits.\nIn the final leg of our survey, we turn our attention to five demographic questions. These demographic data points - income, gender, marital status, location, and age - are the pillars upon which we build a comprehensive understanding of our respondents. Through these demographic lenses, we paint a vivid portrait of the diverse tapestry of movie enthusiasts.\nIn sum, this survey isn’t merely a collection of questions; it’s an intricate data tapestry. It’s an exploration of preferences, a journey through habits, and a window into the lives of movie aficionados. With data analytics as our compass, we’re prepared to unveil the hidden gems of consumer behavior and gain profound insights into the world of movie renting.\n\n\nWhat can be done to improve the respondants’ experiences?\nI re-created and structured this survey for maximum internal and external validation in a new version that can be taken and reviewed here.\nIn crafting this survey, I re-engineered and thoughtfully structured it with an unwavering focus on the pursuit of internal and external validation as well as minimizing non-responses. Our commitment to this approach stems from a deep understanding of the nuances involved in data analytics, which has led us to take several strategic measures.\nRecognizing that a significant portion of our respondents are likely to engage with this survey on mobile devices, we’ve implemented a design that optimizes user experience. To enhance clarity and mitigate potential respondent fatigue, the formidable Likert scale with its 12 subquestions has been artfully deconstructed into three distinct sections. By doing so, we encourage respondents to engage with these most thought-provoking questions in digestible portions, reducing the likelihood of survey abandonment.\nMoreover, we’ve positioned these questions at the outset of the survey, capturing the respondent’s attention and fostering engagement right from the start. This not only contributes to the overall quality of the data collected but also sets a captivating tone for the survey experience.\nIn an effort to make the survey more user-friendly, we’ve innovatively transformed Likert scale responses into user-friendly drop-down menus (seen below). This user-centric adaptation simplifies the process of answering, enhancing the overall survey experience.\n\n\n\nDrop Down Image\n\n\nTo further streamline respondent engagement, we’ve strategically incorporated force response questions and implemented skip logic. These features serve as invaluable tools in maintaining a minimal non-response rate while ensuring that the survey adapts to the unique preferences and behavior of each user.\n\n\nTHE END"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Libby Smith: The Holistic Data Scientist",
    "section": "",
    "text": "Ms. Smith is a budding data scientist with a political science undergraduate degree. She is completing a Master of Science in Social Data Analytics and Research with a tentative graduation date of December 2024. She is well-rounded as a candidate for both private and public sector organizations with a mission to grow while in pursuit of results-oriented knowledge. Her years of experience touches on several key industries including foreign policy, state government, litigation, real estate, and e-commerce."
  },
  {
    "objectID": "index.html#more-information",
    "href": "index.html#more-information",
    "title": "Libby Smith: The Holistic Data Scientist",
    "section": "More Information",
    "text": "More Information\nInterests:\n* Political Economy\n* Logistic Regression & Probability Modeling\n* Machine Learning\n\n\nContact Me\nhirelibbysmith@gmail.com\nLinkedIn"
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Google Trends Website VS Rstudio’s gtrendsR package",
    "section": "",
    "text": "Google Trends Website VS Rstudio’s gtrendsR package\n\nGoogle Trends Website\nGoogle Trends is a web service provided by Google that allows users to explore the popularity of specific search terms over time. It provides valuable insights into how often a particular keyword or topic has been searched for on Google’s search engine. Here’s an explanation of how Google Trends extracts and presents search term trends, including its customization options and main functionalities:\n\n\n\nSearch Terms Trump, Biden, and Election since launch of Google Trends\n\n\nData Collection: Google Trends collects data from Google Search queries. It compiles this data anonymously and aggregates it to create a comprehensive dataset.\nTime Period Customization: Users can customize the time period they want to analyze. Google Trends allows you to specify a custom date range, from a few hours to several years. You can select specific start and end dates, making it easy to focus on a particular timeframe.\nData Normalization: Google Trends normalizes data to allow for fair comparisons. It scales search interest data relative to the total search volume, which ensures that results are not influenced by the overall growth in Google searches over time. This normalization helps users understand relative popularity.\nGeographic Customization: Users can choose the geographic region for which they want to view search trends. Google Trends provides data at various levels, from global to city-specific. This feature is useful for analyzing how search interests vary across different locations.\nCategory Filtering: Google Trends categorizes search topics into specific categories. Users can filter search results by these categories, making it easier to explore trends in specific industries or areas of interest.\nRelated Queries and Topics: Google Trends offers insights into related queries and topics. Users can see which search terms are frequently associated with the topic they are exploring. This feature helps identify related trends and keywords.\nReal-Time Data: Google Trends provides real-time data on search trends. Users can see how the popularity of a search term changes over the course of a day, week, or any specified time interval. This is particularly useful for monitoring breaking news or events.\nData Visualization: The service presents data in easy-to-understand visualizations, including line graphs and charts. Users can quickly grasp the popularity trends of their selected search terms.\nExporting Data: Google Trends allows users to export data for further analysis. You can download CSV files containing search interest data for the terms and timeframes you’ve selected.\nComparative Analysis: Users can compare the search interest of multiple terms simultaneously. This feature helps identify which terms are more popular and how they relate to each other.\nIn summary, Google Trends extracts data from Google Search queries, normalizes it for fair comparisons, and offers a range of customization options. Users can explore search term trends based on different timeframes, locations, categories, and related queries. It also provides real-time data, visualization tools, and the ability to export data for in-depth analysis. Google Trends is a valuable tool for marketers, researchers, and anyone interested in understanding search trends and consumer behavior.\n\n\nRstudio’s gtrendsR package\nWhile both Google Trends website and the gtrendsR package in R allow you to access Google Trends data, there are differences in how you interact with and utilize these methods.\n\n# Load package\nlibrary(gtrendsR)\n# Pull Google Trends Data from \"all time\" interval\nBidenTrumpElection = gtrends(c(\"Trump\",\"Biden\",\"election\"), time = \"all\")\n# Graph identical plot seen in web example\npar(family=\"Georgia\")\nplot(BidenTrumpElection)\n\n\n\n\nThe Google Trends website welcomes users with its intuitive, user-friendly interface, akin to an explorer’s trusted map and compass. Here, one can quickly visualize trends, explore geographic variations, and uncover valuable insights with ease.\nThe ‘gtrendsR’ package has optional arguments for many of the same customizations users can change on the website including but not limited to time, geolocation, and search type that the webpage offers. Both have near identical data querying and toggling settings. While both Google Trends website and the gtrendsR package in R allow you to access Google Trends data, there are differences in how you interact with and utilize these methods. Main differences lie in availability of a user-friendly interface verses a programmic one which enable more visualization and computation methods through base R and other R packages in analyzing the trends.\nOn the other hand, the gtrendsR package in R equips adventurers with a toolkit for programmatic exploration. It’s like having a set of powerful scientific instruments at your disposal. This method empowers users with the ability to harness the full analytical prowess of R and other complementary R packages. Whether you seek to uncover hidden trends, apply advanced statistical methods, or create stunning visualizations, the programmatic nature of this package amplifies your analytical capabilities.\nSee an example below. We will create a heat map through the ggplot2 package to extrapolate on correlations.\n\n# Load ggplot2 package\nlibrary(ggplot2)\n\n# Create new list object specific heat map purposes\nBidenTrumpElection2 &lt;- gtrends(c(\"Trump\", \"Biden\", \"election\"), \n                               time = \"today+5-y\", \n                               geo = (\"US\"))\n\n# Convert \"date\" column to POSIXct format\nBidenTrumpElection2$interest_over_time$date &lt;- as.POSIXct(BidenTrumpElection2$interest_over_time$date)\n\n# Replace \"&lt;1\" with 0 in the \"hits\" column\nBidenTrumpElection2$interest_over_time$hits[BidenTrumpElection2$interest_over_time$hits == \"&lt;1\"] &lt;- 0\n\n# Convert \"hits\" to continuous\nBidenTrumpElection2$interest_over_time$hits &lt;- as.numeric(BidenTrumpElection2$interest_over_time$hits)\n\n# Create the heatmap\nheatmap_plot &lt;- ggplot(data = BidenTrumpElection2$interest_over_time, aes(x = date, y = keyword, fill = hits)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Google Trends Heatmap\", x = \"Date\", y = \"Keyword\") +\n  theme_minimal()\n\n# Print the heatmap\nprint(heatmap_plot)\n\n\n\n\n\n\nTHE END"
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Works Published by the Korea Economic Institute of America",
    "section": "",
    "text": "Antitrust Activism for a More Inclusive Economy\nLink\nBuilding a fairer economy will require shifting the balance of power between small businesses and chaebols - and the Moon administration can do more.\n\n\nEnding Gender Inequality for Economic Recovery\nLink\nRates of female employment in professional, technical, and managerial positions are the lowest of the world’s advanced economies.\n\n\nThe Past, Present, and Future of Capital Punishment in South Korea\nLink\nThe National Human Rights Commission of Korea has called for the Korea to join the coalition of OECD states in outlawing state-sanctioned executions."
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Latent Semantic Text Analysis with Quanteda Package in R",
    "section": "",
    "text": "Latent Semantic Text Analysis with Quanteda Package in R\n\nAnalyzing Tweets surrounding the Biden-Xi Summit of 2021\nLatent Semantic Analysis (LSA) is a technique in natural language processing and text analysis that is used to discover the relationships and associations between words and documents. LSA is primarily applied to large collections of textual data, such as a corpus of documents, to uncover hidden patterns and extract semantic information.\nLSA can uncover semantic relationships and associations that go beyond simple keyword matching. It can reduce the “curse of dimensionality” in high-dimensional text data. It is often used for information retrieval, document summarization, and text mining.\nLSA has 3 major assumptions. 1) Documents are non-positional (“bag of words”). The “bag of words” approach assumes that the order of the words does not matter. What matters is only the frequency of the single words. 2) Concepts are understood as patterns of words where certain words often go together in similar documents. 3) Words only have one meaning given the contexts surrounding the patterns of words.\nTo demonstrate LSA, we will implement the technique on text data from 2021 tweets about the Biden and Xi summit.\n\n# We will work with the following packages\nlibrary(quanteda)\n\nPackage version: 3.3.1\nUnicode version: 13.0\nICU version: 69.1\n\n\nParallel computing: 20 of 20 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# The twitter data is coming from a publicly available file on Github\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can see this Twitter (now X) data has 90 columns containing everything about the Tweets from screenname, reply count, and hashtags. Its a detailed dataset. We do need to take some steps prepping the data to make it readable by the quanteda package for LSA. This includes converting them through the tokenization process. Tokenization involves breaking down a text document into individual units or “tokens,” which are typically words, but they can also be subword units like subword pieces in some tokenization models.\nAfter tokenization, the resulting token object is again filtered through a Document Feature Matrix. A Document-Feature Matrix (Also known as Document Term Matrix) that represents the entire corpus of documents. Each row of the DTM corresponds to a document, and each column corresponds to a term (word) in the corpus. The matrix entries represent the frequency of each term in each document or other suitable measures like TF-IDF (Term Frequency-Inverse Document Frequency) scores.\n\n# Create new object containing only the text data from tweets\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\n\nIn LSA text analysis involving larger amounts of text or large corpora, we often would include a step called Singular Value Decomposition (SVD). In the SVD step of LSA, the DFM is factorized into three matrices. These matrices are based on the tokenized terms, and the relationships between documents and terms are captured by the vectors associated with the tokens. Resulting in a reduced dimensional object that requires less computational power. This step is included in the command “textmodel_lsa” from the quanteda package executed in the code below.\nBelow, LSA is applied to the totality of the text data from the Tweets into a document-feature matrix object (sumtwtdfm). The resulting sum_lsa model contains the LSA model of Summit Tweets. The resulting LSA model of the data can then be interpreted based on the a priori needs of the project. We can see from the resulting summary of the LSA object below that the texts contain 145,200 docs (or Tweets in this case), over 160,000 features (AKA dictionary of text), and 10 sk or singular values obtained from the SVD. Singular values play a significant role in Latent Semantic Analysis (LSA) by quantifying the strength of the latent semantic relationships captured during the dimensionality reduction process. The Matrix_low_rank and data statistics components contain the results of the LSA transformation, but they are represented as a large matrix in sparse format. In this case, it’s stored as a dgCMatrix object, which is a sparse matrix type in R. The dimensions of the matrix are not provided in the summary, but it’s quite large, indicating the transformed LSA space.\n\n# Latent Semantic Analysis\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm)\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                     10 -none-    numeric\ndocs               145200 -none-    numeric\nfeatures           160960 -none-    numeric\nmatrix_low_rank 233713920 -none-    numeric\ndata            233713920 dgCMatrix S4     \n\n\nWe can specify LSA analysis to look at specific features of the text. Below, this section again tokenizes the tweet data (sum_twt) and creates a document-feature matrix (tweet_dfm) without punctuation. Remember, tokenization breaks the text into individual words or tokens, and dfm() creates a document-feature matrix where rows represent documents (tweets) and columns represent terms. Here, we will focus the functions of the quanteda package on the hashtags in our Tweets. We create a document-feature matrix (tag_dfm) specifically for hashtags by selecting terms that start with “#”. Then we identify the top 50 most frequently occurring hashtags.\n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 16,029 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 16,019 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"          \"#biden\"          \"#xijinping\"      \"#joebiden\"      \n [5] \"#america\"        \"#americans\"      \"#coronavirus\"    \"#fentanyl\"      \n [9] \"#xi\"             \"#uyghurgenocide\"\n\n\nNow perhaps we want to do a network analysis of hashtag proliferation surrounding the Summit event. The second line below create a feature co-occurrence matrix (FCM) for hashtags, which captures how often hashtags co-occur in tweets. The next line selects hastags that match the top 50 hashtags from the FCM. Lastly, the final line below generates a network plot of the co-occurrence relationships among the top 50 hashtags. min_freq sets a minimum frequency threshold for displaying hashtags, edge_alpha controls edge transparency, and edge_size adjusts edge thickness.\nWe can see the bulk of the Twitter discourse containing hastags consisted of topics pertaining to fentanyl and covid-19. There are three other less-frequently discussed hashtag topics that branched off from this discussion. Biden tags branched off into other discussions surrounding Uyghers and Human Rights. Another branch-off emanates from the tag #China which appears to consist of US-Taiwan relations based on the frequency of the tags #US and #Taiwan. The final branch off is the #ccp tag which is the smallest and is paired with the #xijinping. Through this visualization of the hashtag network, we can see how various topics are become related to the initial discussion and can be seen as a family of topics that naturally arise together in public discourse.\n\nlibrary(\"quanteda.textplots\")\ntag_fcm &lt;- fcm(tag_dfm)\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nWhat if we want to do a network analysis of users in the Twitter discussion? Similar to the hashtag section, we need to create a document-feature matrix (user_dfm) for user mentions by selecting terms that start with “@”. Like before, we identifies the top 50 most frequently mentioned users. Then, create an FCM for user mentions and select user mentions that match the top 50 mentioned users. Finally, we generate a network plot of the co-occurrence relationships among the top 50 mentioned users. min_freq sets a minimum frequency threshold, and other parameters control the appearance of the network plot.\nAs the plot shows, the bulk of the Tweet discussion is surrounding the users @capitalonearena, @nba, @eneskanter, @pelicansnba, and @washwizards\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@joebiden\"        \"@politico\"        \"@eneskanter\"     \n [5] \"@jendeben\"        \"@nwadhams\"        \"@nba\"             \"@washwizards\"    \n [9] \"@pelicansnba\"     \"@capitalonearena\" \"@kevinliptakcnn\"  \"@foxbusiness\"    \n[13] \"@morningsmaria\"   \"@scmpnews\"        \"@uyghur_american\" \"@nytimes\"        \n[17] \"@petermartin_pcm\" \"@nahaltoosi\"      \"@phelimkine\"      \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 741 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 731 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 5)\n\n\n\n\nSo far we have coded to perform text pre-processing, LSA, and network analysis on Twitter data to explore the relationships between hashtags and user mentions in tweets. The result is a visualization of co-occurrence networks for the most frequent hashtags and user mentions, which can help identify trends and connections in the data. Due to the previously mentioned assumptions for LSA, it has limitations, including difficulty handling polysemy (words with multiple meanings) and the inability to capture context-specific information. More advanced techniques like Word Embeddings (e.g., Word2Vec, GloVe) have become popular alternatives in recent years for capturing semantic relationships in text.\n\n\n\nKeyword-in-Context Analysis and X-ray plots with Quanteda Package in R\n\nAnalyzing Presidential Speeches post 1949\nBelow, I demonstrate some additional functionalities of the quanteda package where we analyze and visualize textual data, specifically examining the usage of certain keywords in American presidential speeches. We only want to focus on more recent speeches here, as indicated by the second line of code. The KWIC command performs a Keyword-in-Context (KWIC) analysis on the corpus. It looks for instances of the keyword “american” in the speeches and generates an X-ray plot that shows the distribution and context of the keyword in the text. ‘textplot_xray()’ is a function that visualizes the KWIC results, providing insights into how the keyword is used in the text. The subsequent code uses textplot_xray() to perform KWIC analysis for multiple keywords: “american,” “people,” and “communist.” It compares the usage and context of these keywords in the text. The resulting visualizations show how each keyword is distributed and used in the corpus.\nOverall, the code demonstrates how to use the “quanteda” and “quanteda.textstats” packages for text analysis and visualization, focusing on examining the distribution and context of specific keywords in American presidential speeches. The resulting X-ray plots provide a visual representation of keyword usage in the corpus. It appears entering the word “communist” was redundant as never appears in the subset of inaugeral speeches since 1949.\n\n# Example extracted from https://quanteda.io/articles/pkgdown/examples/plotting.html\n \nlibrary(quanteda.textstats)\ndata_corpus_inaugural_subset &lt;- \ncorpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\ntextplot_xray(\n  kwic(data_corpus_inaugural_subset, pattern = \"american\"),\n  kwic(data_corpus_inaugural_subset, pattern = \"people\"),\n  kwic(data_corpus_inaugural_subset, pattern = \"communist\")\n)\n\nWarning: 'kwic.corpus()' is deprecated. Use 'tokens()' first.\n\nWarning: 'kwic.corpus()' is deprecated. Use 'tokens()' first.\n\nWarning: 'kwic.corpus()' is deprecated. Use 'tokens()' first.\n\n\n\n\n\n\n\n\nTHE END"
  }
]